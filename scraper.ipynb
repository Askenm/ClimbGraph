{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample HTML parsing function (in case of loading from a file or a URL)\n",
    "def parse_html(html_content,page_number):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Create empty lists to store data for each climb type\n",
    "    boulder_problems = []\n",
    "    trad_climbs = []\n",
    "    sport_routes = []\n",
    "    \n",
    "    # Find all table rows in the climb table\n",
    "    rows = soup.find_all('tr')\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) > 0:\n",
    "            climb_name = cells[0].find('a').text\n",
    "            climb_url = 'https://climbing-history.org' + cells[0].find('a')['href']\n",
    "            climb_type = cells[1].text.strip().lower()\n",
    "            grade = cells[2].text\n",
    "            ascents = cells[3].text\n",
    "            \n",
    "            # Store in the appropriate list based on climb type\n",
    "            if 'boulder' in climb_type:\n",
    "                boulder_problems.append([climb_name, grade, ascents, climb_url])\n",
    "            elif 'trad' in climb_type:\n",
    "                trad_climbs.append([climb_name, grade, ascents, climb_url])\n",
    "            elif 'sport' in climb_type:\n",
    "                sport_routes.append([climb_name, grade, ascents, climb_url])\n",
    "\n",
    "    # Convert lists to pandas DataFrames\n",
    "    boulder_df = pd.DataFrame(boulder_problems, columns=['Name', 'Grade', '# Ascents', 'URL'])\n",
    "    trad_df = pd.DataFrame(trad_climbs, columns=['Name', 'Grade', '# Ascents', 'URL'])\n",
    "    sport_df = pd.DataFrame(sport_routes, columns=['Name', 'Grade', '# Ascents', 'URL'])\n",
    "\n",
    "    # total number of climbs\n",
    "    total_climbs = len(boulder_df) + len(trad_df) + len(sport_df)\n",
    "\n",
    "    # If the files exist, append the new data to them else create them\n",
    "    # Save each DataFrame to a separate CSV file\n",
    "    if os.path.exists('data/boulder_problems.csv'):\n",
    "        boulder_df.to_csv('data/boulder_problems.csv', mode='a', header=False, index=False)\n",
    "        trad_df.to_csv('data/trad_climbs.csv', mode='a', header=False, index=False)\n",
    "        sport_df.to_csv('data/sport_routes.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    else:\n",
    "        boulder_df.to_csv('data/boulder_problems.csv', index=False)\n",
    "        trad_df.to_csv('data/trad_climbs.csv', index=False)\n",
    "        sport_df.to_csv('data/sport_routes.csv', index=False)\n",
    "\n",
    "    print(f\"CSV files from page {page_number} saved successfully!\")\n",
    "\n",
    "    return total_climbs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files from page 1 saved successfully!\n",
      "CSV files from page 2 saved successfully!\n",
      "CSV files from page 3 saved successfully!\n",
      "CSV files from page 4 saved successfully!\n",
      "CSV files from page 5 saved successfully!\n",
      "CSV files from page 6 saved successfully!\n",
      "CSV files from page 7 saved successfully!\n",
      "CSV files from page 8 saved successfully!\n",
      "CSV files from page 9 saved successfully!\n",
      "CSV files from page 10 saved successfully!\n",
      "CSV files from page 11 saved successfully!\n",
      "CSV files from page 12 saved successfully!\n",
      "CSV files from page 13 saved successfully!\n",
      "CSV files from page 14 saved successfully!\n",
      "CSV files from page 15 saved successfully!\n",
      "CSV files from page 16 saved successfully!\n",
      "CSV files from page 17 saved successfully!\n",
      "CSV files from page 18 saved successfully!\n",
      "CSV files from page 19 saved successfully!\n",
      "CSV files from page 20 saved successfully!\n",
      "CSV files from page 21 saved successfully!\n",
      "CSV files from page 22 saved successfully!\n",
      "CSV files from page 23 saved successfully!\n",
      "CSV files from page 24 saved successfully!\n",
      "CSV files from page 25 saved successfully!\n",
      "CSV files from page 26 saved successfully!\n",
      "CSV files from page 27 saved successfully!\n",
      "CSV files from page 28 saved successfully!\n",
      "CSV files from page 29 saved successfully!\n",
      "CSV files from page 30 saved successfully!\n",
      "CSV files from page 31 saved successfully!\n",
      "CSV files from page 32 saved successfully!\n",
      "CSV files from page 33 saved successfully!\n",
      "CSV files from page 34 saved successfully!\n",
      "CSV files from page 35 saved successfully!\n",
      "CSV files from page 36 saved successfully!\n",
      "CSV files from page 37 saved successfully!\n",
      "CSV files from page 38 saved successfully!\n",
      "CSV files from page 39 saved successfully!\n",
      "CSV files from page 40 saved successfully!\n",
      "CSV files from page 41 saved successfully!\n",
      "CSV files from page 42 saved successfully!\n",
      "CSV files from page 43 saved successfully!\n",
      "CSV files from page 44 saved successfully!\n",
      "CSV files from page 45 saved successfully!\n",
      "CSV files from page 46 saved successfully!\n",
      "CSV files from page 47 saved successfully!\n",
      "CSV files from page 48 saved successfully!\n",
      "CSV files from page 49 saved successfully!\n",
      "CSV files from page 50 saved successfully!\n",
      "CSV files from page 51 saved successfully!\n",
      "CSV files from page 52 saved successfully!\n",
      "CSV files from page 53 saved successfully!\n",
      "CSV files from page 54 saved successfully!\n",
      "CSV files from page 55 saved successfully!\n",
      "CSV files from page 56 saved successfully!\n",
      "CSV files from page 57 saved successfully!\n",
      "CSV files from page 58 saved successfully!\n",
      "CSV files from page 59 saved successfully!\n",
      "CSV files from page 60 saved successfully!\n",
      "CSV files from page 61 saved successfully!\n",
      "CSV files from page 62 saved successfully!\n",
      "CSV files from page 63 saved successfully!\n",
      "CSV files from page 64 saved successfully!\n",
      "CSV files from page 65 saved successfully!\n",
      "CSV files from page 66 saved successfully!\n",
      "CSV files from page 67 saved successfully!\n",
      "CSV files from page 68 saved successfully!\n",
      "CSV files from page 69 saved successfully!\n",
      "CSV files from page 70 saved successfully!\n",
      "CSV files from page 71 saved successfully!\n",
      "CSV files from page 72 saved successfully!\n",
      "CSV files from page 73 saved successfully!\n",
      "CSV files from page 74 saved successfully!\n",
      "CSV files from page 75 saved successfully!\n",
      "CSV files from page 76 saved successfully!\n",
      "CSV files from page 77 saved successfully!\n",
      "CSV files from page 78 saved successfully!\n",
      "CSV files from page 79 saved successfully!\n",
      "CSV files from page 80 saved successfully!\n",
      "CSV files from page 81 saved successfully!\n",
      "CSV files from page 82 saved successfully!\n",
      "CSV files from page 83 saved successfully!\n",
      "CSV files from page 84 saved successfully!\n",
      "CSV files from page 85 saved successfully!\n",
      "CSV files from page 86 saved successfully!\n",
      "CSV files from page 87 saved successfully!\n",
      "CSV files from page 88 saved successfully!\n",
      "CSV files from page 89 saved successfully!\n",
      "CSV files from page 90 saved successfully!\n",
      "CSV files from page 91 saved successfully!\n",
      "CSV files from page 92 saved successfully!\n",
      "CSV files from page 93 saved successfully!\n",
      "CSV files from page 94 saved successfully!\n",
      "CSV files from page 95 saved successfully!\n",
      "CSV files from page 96 saved successfully!\n",
      "CSV files from page 97 saved successfully!\n",
      "CSV files from page 98 saved successfully!\n",
      "CSV files from page 99 saved successfully!\n",
      "CSV files from page 100 saved successfully!\n",
      "CSV files from page 101 saved successfully!\n",
      "CSV files from page 102 saved successfully!\n",
      "CSV files from page 103 saved successfully!\n",
      "CSV files from page 104 saved successfully!\n",
      "CSV files from page 105 saved successfully!\n",
      "CSV files from page 106 saved successfully!\n",
      "CSV files from page 107 saved successfully!\n",
      "CSV files from page 108 saved successfully!\n",
      "CSV files from page 109 saved successfully!\n",
      "CSV files from page 110 saved successfully!\n",
      "CSV files from page 111 saved successfully!\n",
      "CSV files from page 112 saved successfully!\n",
      "CSV files from page 113 saved successfully!\n",
      "CSV files from page 114 saved successfully!\n",
      "CSV files from page 115 saved successfully!\n",
      "CSV files from page 116 saved successfully!\n",
      "CSV files from page 117 saved successfully!\n",
      "CSV files from page 118 saved successfully!\n",
      "CSV files from page 119 saved successfully!\n",
      "CSV files from page 120 saved successfully!\n",
      "CSV files from page 121 saved successfully!\n",
      "CSV files from page 122 saved successfully!\n",
      "CSV files from page 123 saved successfully!\n",
      "CSV files from page 124 saved successfully!\n",
      "CSV files from page 125 saved successfully!\n",
      "CSV files from page 126 saved successfully!\n",
      "CSV files from page 127 saved successfully!\n",
      "CSV files from page 128 saved successfully!\n",
      "CSV files from page 129 saved successfully!\n",
      "CSV files from page 130 saved successfully!\n",
      "CSV files from page 131 saved successfully!\n",
      "CSV files from page 132 saved successfully!\n",
      "CSV files from page 133 saved successfully!\n",
      "CSV files from page 134 saved successfully!\n",
      "CSV files from page 135 saved successfully!\n",
      "CSV files from page 136 saved successfully!\n",
      "CSV files from page 137 saved successfully!\n",
      "CSV files from page 138 saved successfully!\n",
      "CSV files from page 139 saved successfully!\n",
      "CSV files from page 140 saved successfully!\n",
      "CSV files from page 141 saved successfully!\n",
      "CSV files from page 142 saved successfully!\n",
      "CSV files from page 143 saved successfully!\n",
      "CSV files from page 144 saved successfully!\n",
      "CSV files from page 145 saved successfully!\n",
      "CSV files from page 146 saved successfully!\n",
      "CSV files from page 147 saved successfully!\n",
      "CSV files from page 148 saved successfully!\n",
      "CSV files from page 149 saved successfully!\n",
      "CSV files from page 150 saved successfully!\n",
      "CSV files from page 151 saved successfully!\n",
      "CSV files from page 152 saved successfully!\n",
      "CSV files from page 153 saved successfully!\n",
      "CSV files from page 154 saved successfully!\n",
      "CSV files from page 155 saved successfully!\n",
      "CSV files from page 156 saved successfully!\n",
      "CSV files from page 157 saved successfully!\n",
      "CSV files from page 158 saved successfully!\n",
      "CSV files from page 159 saved successfully!\n",
      "CSV files from page 160 saved successfully!\n",
      "CSV files from page 161 saved successfully!\n",
      "CSV files from page 162 saved successfully!\n",
      "CSV files from page 163 saved successfully!\n",
      "CSV files from page 164 saved successfully!\n",
      "CSV files from page 165 saved successfully!\n",
      "CSV files from page 166 saved successfully!\n",
      "CSV files from page 167 saved successfully!\n",
      "CSV files from page 168 saved successfully!\n",
      "CSV files from page 169 saved successfully!\n",
      "CSV files from page 170 saved successfully!\n",
      "CSV files from page 171 saved successfully!\n",
      "CSV files from page 172 saved successfully!\n",
      "CSV files from page 173 saved successfully!\n",
      "CSV files from page 174 saved successfully!\n",
      "CSV files from page 175 saved successfully!\n",
      "CSV files from page 176 saved successfully!\n",
      "CSV files from page 177 saved successfully!\n",
      "CSV files from page 178 saved successfully!\n",
      "CSV files from page 179 saved successfully!\n",
      "CSV files from page 180 saved successfully!\n",
      "CSV files from page 181 saved successfully!\n",
      "CSV files from page 182 saved successfully!\n",
      "CSV files from page 183 saved successfully!\n",
      "CSV files from page 184 saved successfully!\n",
      "CSV files from page 185 saved successfully!\n",
      "CSV files from page 186 saved successfully!\n",
      "CSV files from page 187 saved successfully!\n",
      "CSV files from page 188 saved successfully!\n",
      "CSV files from page 189 saved successfully!\n",
      "CSV files from page 190 saved successfully!\n",
      "CSV files from page 191 saved successfully!\n",
      "CSV files from page 192 saved successfully!\n",
      "CSV files from page 193 saved successfully!\n",
      "CSV files from page 194 saved successfully!\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://climbing-history.org/climbs?page=\"\n",
    "\n",
    "\n",
    "page = 1\n",
    "total_climbs = 1\n",
    "while total_climbs > 0:\n",
    "    response = requests.get(base_url + str(page))\n",
    "    total_climbs = parse_html(response.content,page_number=page)\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to hash the climber URL and return a climberID\n",
    "def generate_climber_id(url):\n",
    "    return hashlib.md5(url.encode()).hexdigest()\n",
    "\n",
    "# Function to extract climber details and save to CSV\n",
    "def extract_climbers(html_content, climb_id):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Create an empty list to store climber data\n",
    "    climbers_data = []\n",
    "    \n",
    "    # Find the ascent table\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "        \n",
    "        for row in rows:\n",
    "            try:\n",
    "                cells = row.find_all('td')\n",
    "                #return cells\n",
    "                \n",
    "                # Extract data from each cell\n",
    "                climber_name = cells[0].find('a').text\n",
    "                climber_url = 'https://climbing-history.org' + cells[0].find('a')['href']\n",
    "                \n",
    "                status = cells[1].text.strip()\n",
    "            \n",
    "     \n",
    "                ascent_date = cells[2].text.strip()\n",
    "\n",
    "                \n",
    "                # Generate climber ID as a hash of the URL\n",
    "                climber_id = generate_climber_id(climber_url)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Append data to the climbers_data list\n",
    "            climbers_data.append([climb_id, ascent_date,status, climber_name, climber_url, climber_id])\n",
    "    \n",
    "    # Convert the climbers_data list into a Pandas DataFrame\n",
    "    climbers_df = pd.DataFrame(climbers_data, columns=['ClimbID', 'Ascent Date',\"Status\", 'Climber Name', 'Climber URL', 'ClimberID'])\n",
    "    \n",
    "    # Save DataFrame to CSV\n",
    "    csv_filename = f'data/climbers.csv'\n",
    "    if os.path.exists(csv_filename):\n",
    "        climbers_df.to_csv(csv_filename, mode='a', header=False, index=False)\n",
    "        #print(f\"saved {climbers_df.shape[0]} climbers to {csv_filename}\")\n",
    "    else:\n",
    "        climbers_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    #print(f\"CSV saved successfully as {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ids = pd.read_csv('data/climbers.csv')['ClimbID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing boulder_problems.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1454it [10:42,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trad_climbs.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1138it [10:44,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sport_routes.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1184it [11:22,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for files in ['boulder_problems.csv', 'trad_climbs.csv', 'sport_routes.csv']:\n",
    "    print(f\"Processing {files}...\")\n",
    "    df = pd.read_csv(f'data/{files}')\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        climb_id = files[0]+str(index)\n",
    "        if climb_id in processed_ids:\n",
    "            continue\n",
    "        response = requests.get(row['URL'])\n",
    "        cells = extract_climbers(response.content, climb_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"https://climbing-history.org/climb/2269/phenomena\" == \"https://climbing-history.org/climb/2269/phenomena\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boulder | worked'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cells[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/climbers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://climbing-history.org/climb/157/the-ace'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
